{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wewerthonc/stock_prediction_gans/blob/main/stock_market_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "u-SbBzV8P7u6"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "WIuQ_OguQbUj"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/MyDrive')\n",
        "PATH_BASE = '/content/MyDrive/MyDrive/Datasets/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import and Format Data"
      ],
      "metadata": {
        "id": "WQULpGP7tQA_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "z8FQBdZyR5h9"
      },
      "outputs": [],
      "source": [
        "# Read CSV file\n",
        "stock_df = pd.read_csv(PATH_BASE + 'dados_petroleo_2021.csv', usecols = ['Close', 'Date'], index_col = ['Date'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove pandemic values\n",
        "\n",
        "# Select the dates that are not within the specified interval\n",
        "stock_df = stock_df.loc[~stock_df.index.isin(pd.date_range('2020-02-14', '2020-06-01'))]"
      ],
      "metadata": {
        "id": "UOQqWFOMdwQB"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "eBk496GgWuEJ"
      },
      "outputs": [],
      "source": [
        "stock_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "XIM4o2EmRcSK",
        "outputId": "ec3ff376-b18b-48d6-d20d-f8adad8eadaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "             Close\n",
              "count  1738.000000\n",
              "mean     18.925161\n",
              "std       7.324068\n",
              "min       4.200000\n",
              "25%      13.060000\n",
              "50%      18.965000\n",
              "75%      26.100000\n",
              "max      31.120001"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-56ecf8e7-3b82-4937-8740-d8dddc32dc09\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1738.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>18.925161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>7.324068</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>4.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>13.060000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>18.965000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>26.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>31.120001</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56ecf8e7-3b82-4937-8740-d8dddc32dc09')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-56ecf8e7-3b82-4937-8740-d8dddc32dc09 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-56ecf8e7-3b82-4937-8740-d8dddc32dc09');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "stock_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Tuple\n",
        "\n",
        "def train_validation_split(dataset: pd.DataFrame, test_size: float = 0.2) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Split the dataset into training and validation sets.\n",
        "\n",
        "    Args:\n",
        "        dataset (pd.DataFrame): The dataset to be split.\n",
        "        test_size (float, optional): The proportion of the dataset to be used for validation. Defaults to 0.2.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[pd.DataFrame, pd.DataFrame]: The training and validation sets.\n",
        "    \"\"\"\n",
        "    # Calculate the size of the dataset\n",
        "    df_size = len(dataset)\n",
        "\n",
        "    # Determine the number of rows for the training set based on the test_size proportion\n",
        "    slices = int(df_size * (1 - test_size))\n",
        "\n",
        "    # Split the dataset into training and validation sets\n",
        "    train = dataset.iloc[:slices]\n",
        "    validation = dataset.iloc[slices:]\n",
        "\n",
        "    # Return the training and validation sets\n",
        "    return train, validation"
      ],
      "metadata": {
        "id": "z16dwZsmxQ72"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "class DataNormalizer:\n",
        "    def __init__(self):\n",
        "        self.scaler = MinMaxScaler()\n",
        "\n",
        "    def fit_scaler(self, data: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Fit the scaler on the data.\n",
        "\n",
        "        Parameters:\n",
        "            data (numpy.ndarray or pandas.DataFrame): The input data for fitting the scaler.\n",
        "        \"\"\"\n",
        "        self.scaler.fit(data)\n",
        "\n",
        "    def normalize_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Normalize the data using the fitted scaler.\n",
        "\n",
        "        Parameters:\n",
        "            data (numpy.ndarray or pandas.DataFrame): The input data to be normalized.\n",
        "\n",
        "        Returns:\n",
        "            pandas.DataFrame: The normalized data.\n",
        "        \"\"\"\n",
        "        # Store the original index and column names\n",
        "        index = data.index\n",
        "        columns = data.columns\n",
        "\n",
        "        # Transform the data using the scaler\n",
        "        normalized_data = self.scaler.transform(data)\n",
        "\n",
        "        # Create a DataFrame with the normalized data, using the original index and column names\n",
        "        normalized_df = pd.DataFrame(normalized_data, index=index, columns=columns)\n",
        "\n",
        "        return normalized_df\n",
        "\n",
        "    def inverse_normalize_data(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Inverse normalize the data using the fitted scaler.\n",
        "\n",
        "        Parameters:\n",
        "            data (numpy.ndarray): The normalized data to be inverse normalized.\n",
        "\n",
        "        Returns:\n",
        "            pandas.DataFrame: The inverse normalized data with the same index and column names as the input data.\n",
        "        \"\"\"\n",
        "        # Store the original index and column names\n",
        "        index = data.index\n",
        "        columns = data.columns\n",
        "\n",
        "        # Inverse transform the data using the scaler\n",
        "        inverse_normalized_data = self.scaler.inverse_transform(data)\n",
        "\n",
        "        # Create a DataFrame with the inverse normalized data, using the original index and column names\n",
        "        inverse_normalized_df = pd.DataFrame(inverse_normalized_data, index=index, columns=columns)\n",
        "\n",
        "        return inverse_normalized_df"
      ],
      "metadata": {
        "id": "lAnLPmpKtse9"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "er4to_ddhCjc"
      },
      "outputs": [],
      "source": [
        "def get_formatted_dataset(stand_base: pd.DataFrame, lag: int) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Formats the dataset by creating lagged samples.\n",
        "\n",
        "    Args:\n",
        "        stand_base (pd.DataFrame): Input standardized base dataset.\n",
        "        lag (int): Number of lagged samples.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Formatted dataset with lagged samples.\n",
        "    \"\"\"\n",
        "    len_samples = len(stand_base.index)\n",
        "\n",
        "    # Create resulting dataset\n",
        "    formatted_dataset = pd.DataFrame(np.zeros((len_samples - lag * 2 + 1, lag * 2)))\n",
        "\n",
        "    # Set column labels\n",
        "    formatted_dataset.columns = np.arange(0, lag * 2)\n",
        "\n",
        "    for _ in range(0, len_samples - lag * 2 + 1):\n",
        "      # Select the subset of data for the current lagged sample\n",
        "      subset = stand_base.iloc[_: _ + lag * 2, 0]\n",
        "      \n",
        "      # Assign the subset values to the corresponding row in the formatted dataset\n",
        "      formatted_dataset.iloc[_, 0 : lag * 2] = subset\n",
        "\n",
        "    return formatted_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define GAN"
      ],
      "metadata": {
        "id": "hoG_fTIywAkf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import timedelta\n",
        "\n",
        "class GAN:\n",
        "    def __init__(self, latent_dim: int, lag: int):\n",
        "        \"\"\"\n",
        "        Initializes the GAN model.\n",
        "\n",
        "        Args:\n",
        "            latent_dim (int): Dimensionality of the latent space.\n",
        "            lag (int): Lag value used to calculate the output dimension.\n",
        "        \"\"\"\n",
        "        self.latent_dim = latent_dim\n",
        "        self.output_dim = 2 * lag - latent_dim\n",
        "        self.generator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "        self.discriminator_optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.0001)\n",
        "        self.generator = self.build_generator()\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.model = self.build_model()\n",
        "\n",
        "    def build_generator(self) -> tf.keras.Model:\n",
        "        \"\"\"\n",
        "        Builds the generator model.\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: Generator model.\n",
        "        \"\"\"\n",
        "        generator = tf.keras.Sequential([\n",
        "            layers.LSTM(128, activation='tanh', input_shape=(self.latent_dim, 1), return_sequences=True),\n",
        "            layers.LSTM(64, activation='tanh'),\n",
        "            layers.Dense(32, activation='relu'),\n",
        "            layers.Dense(self.output_dim, activation='linear')\n",
        "        ])\n",
        "        return generator\n",
        "\n",
        "    def build_discriminator(self) -> tf.keras.Model:\n",
        "        \"\"\"\n",
        "        Builds the discriminator model.\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: Discriminator model.\n",
        "        \"\"\"\n",
        "        discriminator = tf.keras.Sequential([\n",
        "            layers.LSTM(128, activation='tanh', input_shape=(self.output_dim, 1), return_sequences=True),\n",
        "            layers.Dense(64, activation='relu'),\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        discriminator.compile(loss=\"binary_crossentropy\", optimizer=self.discriminator_optimizer)\n",
        "        discriminator.trainable = False\n",
        "\n",
        "        return discriminator\n",
        "\n",
        "    def build_model(self) -> tf.keras.Model:\n",
        "        \"\"\"\n",
        "        Builds the combined model of generator and discriminator.\n",
        "\n",
        "        Returns:\n",
        "            tf.keras.Model: Combined model.\n",
        "        \"\"\"\n",
        "        gan = tf.keras.Sequential([\n",
        "            self.generator, self.discriminator\n",
        "        ])\n",
        "        gan.compile(loss=\"binary_crossentropy\", optimizer=self.generator_optimizer)\n",
        "        return gan\n",
        "\n",
        "    def train(self, df: pd.DataFrame, n_epochs: int, batch_size: int) -> None:\n",
        "        \"\"\"\n",
        "        Trains the GAN model.\n",
        "\n",
        "        Args:\n",
        "            df (pd.DataFrame): Input data.\n",
        "            n_epochs (int): Number of training epochs.\n",
        "            batch_size (int): Batch size.\n",
        "        \"\"\"\n",
        "        dataset = tf.data.Dataset.from_tensor_slices(df)\n",
        "        dataset_size = len(dataset)\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "\n",
        "            #dataset = dataset.shuffle(dataset_size, reshuffle_each_iteration=True)\n",
        "\n",
        "            for batch in dataset.batch(batch_size, drop_remainder=True):\n",
        "\n",
        "                # phase 1 - Training discriminator\n",
        "                generated_samples = self.generator(batch.numpy()[:, 0: self.latent_dim])\n",
        "\n",
        "                # Concatenate generated and real data\n",
        "                fake_and_real = tf.concat([generated_samples, batch.numpy()[:, self.latent_dim:]], axis=0)\n",
        "                y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)\n",
        "                d_loss = self.discriminator.train_on_batch(fake_and_real, y1)\n",
        "\n",
        "                # phase 2 - training the generator\n",
        "                generated_samples = batch.numpy()[:, 0:self.latent_dim]\n",
        "                y2 = tf.constant([[1.]] * batch_size)\n",
        "                g_loss = self.model.train_on_batch(generated_samples, y2)\n",
        "\n",
        "                print(f\"Epoch: {epoch+1}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")\n",
        "\n",
        "    def predict(self, df: pd.DataFrame, column: str, days: int) -> pd.DataFrame:\n",
        "      \"\"\"\n",
        "      Predict future values based on the provided DataFrame.\n",
        "\n",
        "      Args:\n",
        "          df (pd.DataFrame): The input DataFrame containing historical data.\n",
        "          column (str): The column name for which predictions are to be made.\n",
        "          days (int): The number of days for which predictions are to be made.\n",
        "\n",
        "      Returns:\n",
        "          pd.DataFrame: The DataFrame with the predicted values appended.\n",
        "\n",
        "      \"\"\"\n",
        "\n",
        "      # Create new values\n",
        "      for _ in range(0, days):\n",
        "          # Select the last rows of the 'Close' column\n",
        "          close_subset = df[column].iloc[-self.latent_dim:]\n",
        "\n",
        "          # Reshape the subset into a 1-row array\n",
        "          reshaped_array = close_subset.values.reshape(1, self.latent_dim)\n",
        "\n",
        "          # Generate the prediction using the reshaped array\n",
        "          prediction = gan.generator.predict(reshaped_array)\n",
        "\n",
        "          # Get the last date from the 'Date' column\n",
        "          last_date = df['Date'].iloc[-1]\n",
        "\n",
        "          # Calculate the next date by adding one day\n",
        "          next_date = last_date + timedelta(days=1)\n",
        "\n",
        "          # Print the next date\n",
        "          print(\"Next date:\", next_date.date())\n",
        "\n",
        "          # Create a new row with the next date and the predicted value\n",
        "          new_row = pd.DataFrame({'Date': [next_date], column: [prediction[0][0]]})\n",
        "\n",
        "          # Append the new row to the DataFrame\n",
        "          df = pd.concat([df, new_row], ignore_index = True)\n",
        "\n",
        "      return df"
      ],
      "metadata": {
        "id": "kJ2bbfy5tQAJ"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tuning"
      ],
      "metadata": {
        "id": "kggJKZZfxYfc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def set_seeds(seed: int) -> None:\n",
        "    \"\"\"\n",
        "    Sets the seed values for numpy, random, and TensorFlow.\n",
        "\n",
        "    Args:\n",
        "        seed (int): Seed value to set.\n",
        "    \"\"\"\n",
        "    # Set seed for numpy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Set seed for random module\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Set seed for TensorFlow\n",
        "    tf.random.set_seed(seed)"
      ],
      "metadata": {
        "id": "3w4mJAG_Wl4s"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "def save_print_to_file(text, filename):\n",
        "    \"\"\"\n",
        "    Saves the text to a file.\n",
        "\n",
        "    Args:\n",
        "        text (str): Text to be saved.\n",
        "        filename (str): Name of the file to save.\n",
        "    \"\"\"\n",
        "    with open(filename, 'a') as file:\n",
        "        sys.stdout = file\n",
        "        print(text)\n",
        "        sys.stdout = sys.__stdout__"
      ],
      "metadata": {
        "id": "NDnG3Puv0yZg"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def tune_gan_model(train_df: pd.DataFrame, val_df: pd.DataFrame, hyperparameters: Dict) -> None:\n",
        "  \"\"\"\n",
        "    Tune a GAN model using the provided hyperparameters.\n",
        "\n",
        "    Args:\n",
        "        df (pandas.DataFrame): The input dataset.\n",
        "        hyperparameters (dict or iterable): Hyperparameters to be tuned.\n",
        "\n",
        "  \"\"\"\n",
        "  best_mse = np.inf\n",
        "  best_params = None\n",
        "\n",
        "  # Iterate over different combinations of hyperparameters\n",
        "  for params in ParameterGrid(hyperparameters):\n",
        "  \n",
        "      # Set random seed\n",
        "      set_seeds(42)\n",
        "    \n",
        "      # Extract hyperparameters from the current combination\n",
        "      latent_dim = params['latent_dim']\n",
        "      epochs = params['epochs']\n",
        "      batch_size = params['batch_size']\n",
        "      lag = params['lag']\n",
        "\n",
        "      # Create Data Normalizer\n",
        "      normalizer = DataNormalizer()\n",
        "      normalizer.fit_scaler(train_df)\n",
        "      \n",
        "      # Create a GAN model instance\n",
        "      gan = GAN(latent_dim, lag)\n",
        "\n",
        "      # Scale the training dataset\n",
        "      normalized_train = normalizer.normalize_data(train_df)\n",
        "\n",
        "      # Prepare the training dataset\n",
        "      formatted_train_dataset = get_formatted_dataset(normalized_train, lag)\n",
        "\n",
        "      # Train the GAN model\n",
        "      gan.train(formatted_train_dataset, epochs, batch_size)\n",
        "\n",
        "      # Scale the validation dataset\n",
        "      normalized_val = normalizer.normalize_data(val_df)\n",
        "    \n",
        "      # Calculate mean squared error for validation dataset\n",
        "      mse_list = []\n",
        "\n",
        "      # Prepare the validation dataset\n",
        "      formatted_val_dataset = get_formatted_dataset(normalized_val, lag)\n",
        "      dataset = tf.data.Dataset.from_tensor_slices(formatted_val_dataset)\n",
        "      \n",
        "      # Iterate over batches of the validation dataset\n",
        "      for batch in dataset.batch(batch_size, drop_remainder=True):\n",
        "          generated_samples = gan.generator(batch.numpy()[:, 0:latent_dim])\n",
        "          mse = mean_squared_error(generated_samples, batch.numpy()[:, latent_dim:])\n",
        "          mse_list.append(mse)\n",
        "    \n",
        "      # Calculate average mean squared error\n",
        "      avg_mse = np.mean(mse_list)\n",
        "    \n",
        "      # Prepare parameters and save/print to file\n",
        "      parameters = f\"\\nParameters: {params} \" + f\"Average MSE: {avg_mse}\"\n",
        "      filename = \"/content/MyDrive/MyDrive/Datasets/output.txt\"\n",
        "      save_print_to_file(parameters, filename)\n",
        "\n",
        "      # Update best mean squared error and corresponding hyperparameters if necessary\n",
        "      if avg_mse < best_mse:\n",
        "          best_mse = avg_mse\n",
        "          best_params = params\n",
        "\n",
        "  # Prepare best parameters and save/print to file\n",
        "  parameters = f\"\\nBest Parameters: {best_params} \" + f\"Best Average MSE: {best_mse}\"\n",
        "  filename = \"/content/MyDrive/MyDrive/Datasets/output.txt\"\n",
        "  save_print_to_file(parameters, filename)"
      ],
      "metadata": {
        "id": "UKRvveMXkgOz"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "    'latent_dim': [90],\n",
        "    'epochs': [200],\n",
        "    'batch_size': [16],\n",
        "    'lag': [90]\n",
        "}"
      ],
      "metadata": {
        "id": "xXgXFqRHknDh"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_validation_split(stock_df)\n",
        "len(train_df), len(val_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6u2MtrU0Ff0",
        "outputId": "b2f8dd6b-294c-4935-c5c9-cf4210218eab"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1390, 348)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tune_gan_model(train_df, val_df, hyperparameters)"
      ],
      "metadata": {
        "id": "zW2FDhFXzy1p"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prediction"
      ],
      "metadata": {
        "id": "vM-bB2ITwk0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_dataframe_to_csv(dataframe: pd.DataFrame, filepath: str) -> None:\n",
        "    \"\"\"\n",
        "    Save a DataFrame to a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "        dataframe (pandas.DataFrame): The DataFrame to be saved.\n",
        "        filepath (str): The file path where the CSV file will be saved.\n",
        "    \"\"\"\n",
        "    dataframe.to_csv(filepath, index=False)"
      ],
      "metadata": {
        "id": "qxQqJaZIGOJJ"
      },
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed\n",
        "set_seeds(42)"
      ],
      "metadata": {
        "id": "mZfKsq2pheQj"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the dimensionality of the latent space\n",
        "latent_dim = 90\n",
        "\n",
        "# Define the lag for the time series data\n",
        "lag = 90\n",
        "\n",
        "# Set the batch size for training\n",
        "batch_size = 16\n",
        "\n",
        "# Set the number of epochs for training\n",
        "epochs = 200\n",
        "\n",
        "# Create an instance of the DataNormalizer class\n",
        "normalizer = DataNormalizer()\n",
        "\n",
        "# Fit the scaler on the stock data to compute normalization parameters\n",
        "normalizer.fit_scaler(stock_df)\n",
        "\n",
        "# Normalize the stock data using the computed parameters\n",
        "df_norm = normalizer.normalize_data(stock_df)\n",
        "\n",
        "# Create an instance of the GAN class\n",
        "gan = GAN(latent_dim, lag)\n",
        "\n",
        "# Get the formatted dataset for training the GAN\n",
        "formatted_df = get_formatted_dataset(df_norm, lag)\n",
        "\n",
        "# Train the GAN model on the formatted dataset\n",
        "gan.train(formatted_df, epochs, batch_size)\n",
        "\n",
        "# Reset the index of the normalized dataset and convert the 'Date' column to datetime format\n",
        "dataset = df_norm.reset_index()\n",
        "dataset['Date'] = pd.to_datetime(dataset['Date'])\n",
        "\n",
        "# Generate predictions using the trained GAN model\n",
        "predictions = gan.predict(dataset, 'Close', 90).set_index('Date')\n",
        "\n",
        "# Inverse normalize the predictions to obtain the original scale\n",
        "predictions_corrected = normalizer.inverse_normalize_data(predictions)"
      ],
      "metadata": {
        "id": "YQ3PGaWwxL5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "# Set seaborn style and colors\n",
        "sns.set_style('whitegrid')\n",
        "colors = [\"#DE8F8F\", \"#8FB8DE\"]\n",
        "\n",
        "# Select the last 120 values from the 'Close' column\n",
        "subset = predictions_corrected['Close'].tail(120)\n",
        "\n",
        "# Set up the plot figure and axes\n",
        "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "ax2 = ax1.twinx()\n",
        "\n",
        "# Plot the first half of the time series in the first color\n",
        "sns.lineplot(data=subset.iloc[:30], palette=[colors[0]], ax=ax1)\n",
        "\n",
        "# Plot the second half of the time series in the second color\n",
        "sns.lineplot(data=subset.iloc[30:], palette=[colors[1]], ax=ax2)\n",
        "\n",
        "# Set the prediction start date\n",
        "prediction_start_date = datetime.datetime(2022, 1, 1)\n",
        "\n",
        "# Find the index of the prediction start date in the subset DataFrame\n",
        "start_date_index = subset.index.get_loc(prediction_start_date)\n",
        "\n",
        "# Add a vertical line at the prediction start date\n",
        "ax1.axvline(x=subset.index[start_date_index], color='red', linestyle='--', label='Prediction Start')\n",
        "\n",
        "# Set the plot title\n",
        "plt.title('Stock Price Trend')\n",
        "\n",
        "# Set the labels for each axis\n",
        "ax1.set_xlabel('Date')\n",
        "ax1.set_ylabel('Price (First Half)')\n",
        "ax2.set_ylabel('Price Prediction')\n",
        "\n",
        "# Set the x-axis range to cover the first and last date of the dataset\n",
        "ax1.set_xlim(subset.index[0], subset.index[-1])\n",
        "\n",
        "# Set the y-axis range between 0 and 30\n",
        "ax1.set_ylim(15, 30)\n",
        "\n",
        "# Set the y-axis range between 0 and 30\n",
        "ax2.set_ylim(15, 30)\n",
        "\n",
        "# Rotate x-axis labels by 45 degrees\n",
        "ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45)\n",
        "\n",
        "# Show the legends\n",
        "ax1.legend(loc='upper left')\n",
        "ax2.legend(loc='upper right')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gm62YZ6-LSaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dataframe_to_csv(predictions_corrected, '/content/MyDrive/MyDrive/Datasets/predictions_corrected.csv')"
      ],
      "metadata": {
        "id": "tuSYzLV3WyBw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPj4VBNOcwPPXv90RKzzKhG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}